{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTING THE LIBRARIES "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. DATA HANDLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import array, asarray, zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten, GlobalMaxPool1D, Conv1D, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import average_precision_score, f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. SAVING THE MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading preprocessed dataset\n",
    "file_path = \"/Users/lalitaneeharikavajjhala/Desktop/Research credits /Data/preprocessed_data.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['obligation']</td>\n",
       "      <td>we will issue a certificate of completion for each manager trainee who completes the initial training program we require to our satisfaction each such person will be referred to a a certified manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['obligation']</td>\n",
       "      <td>elephant talk bear the risk of and shall indemnify against high usage fraud and bed of it elephant talk customer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['obligation']</td>\n",
       "      <td>subject to the term and condition of this agreement aimmune shall be responsible for the development of the product a set forth herein aimmune itself or with or through it affiliate and sublicensees shall use commercially reasonable effort to perform the development activity for the product to i achieve the development milestone set forth in section and ii obtain regulatory approval for the product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['obligation']</td>\n",
       "      <td>ediets shall ensure that the ediets content complies with editorial guideline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['obligation']</td>\n",
       "      <td>auriemma will participate in one recording session annually during the service period of not more than two hour not including travel time to record a radio advertising spot at a date and location to be mutually agreed upon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tag  \\\n",
       "0  ['obligation']   \n",
       "1  ['obligation']   \n",
       "2  ['obligation']   \n",
       "3  ['obligation']   \n",
       "4  ['obligation']   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                            sentence  \n",
       "0                                                                                                                                                                                                            we will issue a certificate of completion for each manager trainee who completes the initial training program we require to our satisfaction each such person will be referred to a a certified manager  \n",
       "1                                                                                                                                                                                                                                                                                                   elephant talk bear the risk of and shall indemnify against high usage fraud and bed of it elephant talk customer  \n",
       "2  subject to the term and condition of this agreement aimmune shall be responsible for the development of the product a set forth herein aimmune itself or with or through it affiliate and sublicensees shall use commercially reasonable effort to perform the development activity for the product to i achieve the development milestone set forth in section and ii obtain regulatory approval for the product  \n",
       "3                                                                                                                                                                                                                                                                                                                                      ediets shall ensure that the ediets content complies with editorial guideline  \n",
       "4                                                                                                                                                                                     auriemma will participate in one recording session annually during the service period of not more than two hour not including travel time to record a radio advertising spot at a date and location to be mutually agreed upon  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Previewing data head and extend the max column width\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting tags from strings to lists\n",
    "df['tag'] = df['tag'].apply(lambda x: literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding tags 'y'\n",
    "y = df['tag']\n",
    "multilabel = MultiLabelBinarizer()\n",
    "y = multilabel.fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard keras pre-processing\n",
    "maxlen = 200 # Highest word count is 691 and mean is 52; however, 691 is an outlier\n",
    "max_words = 2000\n",
    "tokenizer = Tokenizer(num_words=max_words, lower=True)\n",
    "tokenizer.fit_on_texts(df.sentence)\n",
    "\n",
    "# Functions to transform text to feature_vectors \n",
    "def get_features(text_series):\n",
    "    sequences = tokenizer.texts_to_sequences(text_series)\n",
    "    return pad_sequences(sequences, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(947, 200) (947, 3)\n"
     ]
    }
   ],
   "source": [
    "# Calling function to create features 'X'\n",
    "X = get_features(df.sentence)\n",
    "\n",
    "# Transforming y\n",
    "y = multilabel.transform(df.tag)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   8, 577, 372],\n",
       "       [  0,   0,   0, ..., 105, 106, 109],\n",
       "       [  0,   0,   0, ...,  19,   1,  31],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,  14,  11,  37],\n",
       "       [  0,   0,   0, ...,   1,  12,   9],\n",
       "       [  0,   0,   0, ..., 276,   5, 238]], dtype=int32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# law2vec 100 dimensional word embeddings\n",
    "vocab_size = len(tokenizer.word_index) + 1 # Adding 1 accounts for the possibility of having an out-of-vocabulary token\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "law2vec_file = open(\"/Users/lalitaneeharikavajjhala/Desktop/Research credits /Data/Law2Vec.100d.txt\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing each line and storing word-vector pairs in a dictionary\n",
    "for line in law2vec_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "law2vec_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each row corresponds to a word with its 100 dimensional word vector\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "\n",
    "# tokenizer.word_index is a list of (word, id) tuples\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL DEVELOPMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 200, 100)          292700    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 200, 100)          0         \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 198, 300)          90300     \n",
      "                                                                 \n",
      " global_max_pooling1d_3 (Gl  (None, 300)               0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 903       \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 383903 (1.46 MB)\n",
      "Trainable params: 91203 (356.26 KB)\n",
      "Non-trainable params: 292700 (1.12 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Modelling - Convolutional Neural Network with law2vec embedding\n",
    "\n",
    "filter_length = 300\n",
    "num_classes = 3 #so that the final layer is capable of outputting multiple labels\n",
    "\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen, trainable=False)\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv1D(filter_length, 3, padding='valid', activation='relu', strides=1))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/24 [=========================>....] - ETA: 0s - loss: 0.6557 - categorical_accuracy: 0.4003WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.6508 - categorical_accuracy: 0.4188 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.5722 - categorical_accuracy: 0.6652WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.5673 - categorical_accuracy: 0.6777 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.4984 - categorical_accuracy: 0.7545WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.4950 - categorical_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.4267 - categorical_accuracy: 0.8229WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.4284 - categorical_accuracy: 0.8151 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.3544 - categorical_accuracy: 0.8899WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.3537 - categorical_accuracy: 0.8904 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.2982 - categorical_accuracy: 0.8869WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.2969 - categorical_accuracy: 0.8864 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.2533 - categorical_accuracy: 0.8899WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.2505 - categorical_accuracy: 0.8864 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.2140 - categorical_accuracy: 0.9003WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.2147 - categorical_accuracy: 0.8943 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.1860 - categorical_accuracy: 0.9033WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1880 - categorical_accuracy: 0.9009 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.1692 - categorical_accuracy: 0.9003WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1655 - categorical_accuracy: 0.9009 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.1407 - categorical_accuracy: 0.9182WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1446 - categorical_accuracy: 0.9115 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.1252 - categorical_accuracy: 0.9107WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.1271 - categorical_accuracy: 0.9049 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.1135 - categorical_accuracy: 0.9315WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.1140 - categorical_accuracy: 0.9273 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.1025 - categorical_accuracy: 0.9330WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.1042 - categorical_accuracy: 0.9287 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.0893 - categorical_accuracy: 0.9313WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0893 - categorical_accuracy: 0.9313 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0857 - categorical_accuracy: 0.9315WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0870 - categorical_accuracy: 0.9300 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0742 - categorical_accuracy: 0.9301WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0731 - categorical_accuracy: 0.9287 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0679 - categorical_accuracy: 0.9315WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0678 - categorical_accuracy: 0.9326 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0603 - categorical_accuracy: 0.9301WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0598 - categorical_accuracy: 0.9300 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "21/24 [=========================>....] - ETA: 0s - loss: 0.0559 - categorical_accuracy: 0.9330WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,categorical_accuracy,lr\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0547 - categorical_accuracy: 0.9300 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(), # This callback reduces the learning rate when a monitored metric has stopped improving\n",
    "    EarlyStopping(patience=4)\n",
    "]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step - loss: 0.2091 - categorical_accuracy: 0.8737\n",
      "loss: 0.20908187329769135\n",
      "categorical_accuracy: 0.8736842274665833\n"
     ]
    }
   ],
   "source": [
    "# Evaluation metrics\n",
    "metrics = model.evaluate(X_test, y_test)\n",
    "print(\"{}: {}\".format(model.metrics_names[0], metrics[0]))\n",
    "print(\"{}: {}\".format(model.metrics_names[1], metrics[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step\n",
      "Precision: 0.9053182330863692\n",
      "Recall: 0.8651162790697674\n",
      "F1 Score: 0.8839369000968755\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "thresholded_preds = (y_pred > 0.5).astype(int)  # Applying threshold for binary classification\n",
    "precision = precision_score(y_test, thresholded_preds, average = 'weighted')\n",
    "recall = recall_score(y_test, thresholded_preds, average = 'weighted')\n",
    "f1 = f1_score(y_test, thresholded_preds, average= 'weighted')\n",
    "# print(\"Precision Score: {:.2}\".format(average_precision_score(y_test,y_pred)))\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 8ms/step\n",
      "[[0.9671399  0.0086486  0.61855996]]\n",
      "[('obligation', 'prohibition')]\n"
     ]
    }
   ],
   "source": [
    "# x = [\"Each Party shall return to the other all of the other’s Confidential Information and any other material, information or samples relating to the Product which have been provided or made available to the other and shall not retain any copies and the Parties further agree not to make any further use of each other’s Confidential Information or any other information, data or samples relating to the Product provided or made available by the other Party, except as necessary to comply with its statutory, regulatory or licensing obligations; provided, however, that Kitov may retain such material, information and/or samples relating to the Product as may be necessary for Kitov to continue to sell the Product as permitted by Section ​5.4.4 below, following which, Kitov shall refrain from making any further use of Dexcel’s Confidential Information or any other information, data or samples and shall return any remaining Confidential Information and material, information or samples relating to the Product.\"]\n",
    "x = [\"The confidentiality obligations contained in this section XI shall not apply to the extent that the receiving Party (the 'Recipient') is required (a) to disclose information by law, order or regulation of a governmental agency or a court of competent jurisdiction , or (b) to disclose information to any governmental agency for purposes of obtaining approval to test or market a Product , provided in either case that the Recipient shall provide written notice thereof to the other Party and sufficient opportunity to object to any such disclosure or to request confidential treatment thereof.\"]\n",
    "xt = get_features(x)\n",
    "prediction = model.predict(xt)\n",
    "# probas = np.array(prediction)\n",
    "# labels = (probas > 0.5).astype(np.int)\n",
    "\n",
    "probas = (prediction > 0.5).astype(int)\n",
    "tags = multilabel.inverse_transform(probas)\n",
    "# tags = multilabel.inverse_transform(labels)\n",
    "\n",
    "print(prediction)\n",
    "# print(labels)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVING THE MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer\n",
    "# joblib.dump(tokenizer, \"/Users/lalitaneeharikavajjhala/Desktop/Research credits /Models/MultiLabelTokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save binarizer\n",
    "# joblib.dump(multilabel, \"/Users/lalitaneeharikavajjhala/Desktop/Research credits /Models/MultiLabelBinarizer_CNN.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "# joblib.dump(model, \"/Users/lalitaneeharikavajjhala/Desktop/Research credits /Models/MultiLabelModel_CNN.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
