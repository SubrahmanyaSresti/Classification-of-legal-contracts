{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0669f227-d367-4ee7-a7f7-999f648de517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 766\n",
      "Dev size: 86\n",
      "Test size: 95\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "df= pd.read_csv(\"preprocessed_data.csv\")\n",
    "df.groupby('tag').describe()\n",
    "labels=[]\n",
    "for i in range(len(df)):\n",
    "    temp=''\n",
    "    if('permission'in df['tag'][i]):\n",
    "        labels.append('yes')\n",
    "    else:\n",
    "        labels.append('no')\n",
    "rest_texts, test_texts, rest_labels, test_labels = train_test_split(df['sentence'], labels, test_size=0.1, random_state=1)\n",
    "train_texts, dev_texts, train_labels, dev_labels = train_test_split(rest_texts, rest_labels, test_size=0.1, random_state=1)\n",
    "\n",
    "print(\"Train size:\", len(train_texts))\n",
    "print(\"Dev size:\", len(dev_texts))\n",
    "print(\"Test size:\", len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87eec72b-bcf1-43c4-b89a-a9cc29aa56a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yes': 0, 'no': 1}\n"
     ]
    }
   ],
   "source": [
    "target_names = list(set(labels))\n",
    "label2idx = {label: idx for idx, label in enumerate(target_names)}\n",
    "print(label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d20ce2b4-4e45-4e15-9ced-0d1fb3214854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Baseline accuracy: 0.8631578947368421\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('lr', LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\"))\n",
    "])\n",
    "\n",
    "parameters = {'lr__C': [0.1, 0.5, 1, 2, 5, 10, 100, 1000]}\n",
    "\n",
    "best_classifier = GridSearchCV(pipeline, parameters, cv=5, verbose=1)\n",
    "best_classifier.fit(train_texts, train_labels)\n",
    "best_predictions = best_classifier.predict(test_texts)\n",
    "\n",
    "baseline_accuracy = np.mean(best_predictions == test_labels)\n",
    "print(\"Baseline accuracy:\", baseline_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3048c04-e73e-42c3-81f8-fe16a5cc9249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c658fd45-14c6-4a08-b6ad-4adb17d50753",
   "metadata": {},
   "source": [
    "### Using the bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00ccc558-a021-42e8-9dde-471fc48c30a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/bert-base-uncased-contracts and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BERT_MODEL = \"nlpaueb/bert-base-uncased-contracts\"\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(BERT_MODEL, num_labels = len(label2idx))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9bb4dac-ed71-4683-8eaf-7b367550da77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (710 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MAX_SEQ_LENGTH=100\n",
    "\n",
    "class BertInputItem(object):\n",
    "    \"\"\"An item with all the necessary attributes for finetuning BERT.\"\"\"\n",
    "\n",
    "    def __init__(self, text, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.text = text\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        \n",
    "\n",
    "def convert_examples_to_inputs(example_texts, example_labels, label2idx, max_seq_length, tokenizer, verbose=0):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "    \n",
    "    input_items = []\n",
    "    examples = zip(example_texts, example_labels)\n",
    "    for (ex_index, (text, label)) in enumerate(examples):\n",
    "\n",
    "        # Create a list of token ids\n",
    "        input_ids = tokenizer.encode(f\"[CLS] {text} [SEP]\")\n",
    "        if len(input_ids) > max_seq_length:\n",
    "            input_ids = input_ids[:max_seq_length]\n",
    "\n",
    "        # All our tokens are in the first input segment (id 0).\n",
    "        segment_ids = [0] * len(input_ids)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        label_id = label2idx[label]\n",
    "\n",
    "        input_items.append(\n",
    "            BertInputItem(text=text,\n",
    "                          input_ids=input_ids,\n",
    "                          input_mask=input_mask,\n",
    "                          segment_ids=segment_ids,\n",
    "                          label_id=label_id))\n",
    "\n",
    "        \n",
    "    return input_items\n",
    "\n",
    "train_features = convert_examples_to_inputs(train_texts, train_labels, label2idx, MAX_SEQ_LENGTH, tokenizer, verbose=0)\n",
    "dev_features = convert_examples_to_inputs(dev_texts, dev_labels, label2idx, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = convert_examples_to_inputs(test_texts, test_labels, label2idx, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0bff4bd-674c-400e-9cf1-8d2829213445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "\n",
    "def get_data_loader(features, max_seq_length, batch_size, shuffle=True): \n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "    data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "    dataloader = DataLoader(data, shuffle=shuffle, batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataloader = get_data_loader(train_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=True)\n",
    "dev_dataloader = get_data_loader(dev_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = get_data_loader(test_features, MAX_SEQ_LENGTH, BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bad037a8-7528-4c5a-a690-58a80e5e94d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    predicted_labels, correct_labels = [], []\n",
    "\n",
    "    for step, batch in enumerate(tqdm(dataloader, desc=\"Evaluation iteration\")):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss = model(input_ids, attention_mask=input_mask,\n",
    "                                          token_type_ids=segment_ids, labels=label_ids).loss\n",
    "            logits= model(input_ids, attention_mask=input_mask,\n",
    "                                          token_type_ids=segment_ids, labels=label_ids).logits\n",
    "            print(tmp_eval_loss)\n",
    "\n",
    "        outputs = np.argmax(logits.to('cpu'), axis=1)\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        \n",
    "        predicted_labels += outputs\n",
    "        correct_labels += list(label_ids)\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    \n",
    "    correct_labels = np.array(correct_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "        \n",
    "    return eval_loss, correct_labels , predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d66e2247-fa94-48bb-87db-6766fb156d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfsshare/mss-n15/.conda/envs/myenv/lib/python3.12/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers.optimization import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "NUM_TRAIN_EPOCHS = 20\n",
    "LEARNING_RATE = 5e-5\n",
    "WARMUP_PROPORTION = 0.1\n",
    "MAX_GRAD_NORM = 5\n",
    "\n",
    "num_train_steps = int(len(train_dataloader.dataset) / BATCH_SIZE / GRADIENT_ACCUMULATION_STEPS * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(WARMUP_PROPORTION * num_train_steps)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, correct_bias=False)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps, num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a54b705d-3261-4875-be63-7bbf471c7155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|                                             | 0/20 [00:00<?, ?it/s]/tmp/ipykernel_29754/1537822491.py:17: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training iteration\")):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff4b579986a4c1bbacaaee5a9618d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training iteration:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29754/1871874811.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for step, batch in enumerate(tqdm(dataloader, desc=\"Evaluation iteration\")):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af10c603bdd3477f9ab780215c2def74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation iteration:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0344, device='cuda:0')\n",
      "tensor(0.3329, device='cuda:0')\n",
      "tensor(0.2334, device='cuda:0')\n",
      "tensor(0.0631, device='cuda:0')\n",
      "tensor(0.5825, device='cuda:0')\n",
      "tensor(0.0303, device='cuda:0')\n",
      "Loss history: []\n",
      "Dev loss: 0.21276293819149336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   5%|█▊                                   | 1/20 [00:05<01:37,  5.12s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09abc52cf34a48369128a6eb53fc3323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training iteration:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507ceeea8e364ac8831e962995ad1031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation iteration:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0394, device='cuda:0')\n",
      "tensor(0.6122, device='cuda:0')\n",
      "tensor(0.8091, device='cuda:0')\n",
      "tensor(0.0245, device='cuda:0')\n",
      "tensor(0.7108, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|███▋                                 | 2/20 [00:09<01:21,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1737, device='cuda:0')\n",
      "Loss history: [0.21276293819149336]\n",
      "Dev loss: 0.3949471547578772\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5528b264576748c083c8aed2ad18c18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training iteration:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f3e03d289e41958cc7c33b9f1de402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation iteration:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0160, device='cuda:0')\n",
      "tensor(0.2717, device='cuda:0')\n",
      "tensor(0.0190, device='cuda:0')\n",
      "tensor(0.0139, device='cuda:0')\n",
      "tensor(0.3284, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|███▋                                 | 2/20 [00:13<01:57,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0159, device='cuda:0')\n",
      "Loss history: [0.21276293819149336, 0.3949471547578772]\n",
      "Dev loss: 0.11081688556199272\n",
      "No improvement on development set. Finish training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "OUTPUT_DIR = \"/tmp/\"\n",
    "MODEL_FILE_NAME = \"pytorch_model.bin\"\n",
    "PATIENCE = 2\n",
    "\n",
    "loss_history = []\n",
    "no_improvement = 0\n",
    "for _ in trange(int(NUM_TRAIN_EPOCHS), desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training iteration\")):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=input_mask, token_type_ids=segment_ids, labels=label_ids)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        if GRADIENT_ACCUMULATION_STEPS > 1:\n",
    "            loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "        loss.backward()\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)  \n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            \n",
    "    dev_loss, _, _ = evaluate(model, dev_dataloader)\n",
    "    \n",
    "    print(\"Loss history:\", loss_history)\n",
    "    print(\"Dev loss:\", dev_loss)\n",
    "    \n",
    "    if len(loss_history) == 0 : #or dev_loss < min(loss_history):\n",
    "        no_improvement = 0\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model\n",
    "        output_model_file = os.path.join(OUTPUT_DIR, MODEL_FILE_NAME)\n",
    "        torch.save(model_to_save.state_dict(), output_model_file)\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "    \n",
    "    if no_improvement >= PATIENCE: \n",
    "        print(\"No improvement on development set. Finish training.\")\n",
    "        break\n",
    "        \n",
    "    \n",
    "    loss_history.append(dev_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "abe37a00-4433-481c-ae75-9f94090a108d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29754/1871874811.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for step, batch in enumerate(tqdm(dataloader, desc=\"Evaluation iteration\")):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3f0e9cb83441148b53c1752f654482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation iteration:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0306, device='cuda:0')\n",
      "tensor(0.0955, device='cuda:0')\n",
      "tensor(0.0477, device='cuda:0')\n",
      "tensor(0.2375, device='cuda:0')\n",
      "tensor(0.1505, device='cuda:0')\n",
      "tensor(0.2611, device='cuda:0')\n",
      "tensor(0.0340, device='cuda:0')\n",
      "tensor(0.2295, device='cuda:0')\n",
      "tensor(0.5011, device='cuda:0')\n",
      "tensor(0.2887, device='cuda:0')\n",
      "tensor(0.5448, device='cuda:0')\n",
      "tensor(0.0269, device='cuda:0')\n",
      "tensor(0.0796, device='cuda:0')\n",
      "tensor(0.0330, device='cuda:0')\n",
      "tensor(0.0488, device='cuda:0')\n",
      "tensor(0.0822, device='cuda:0')\n",
      "tensor(0.2355, device='cuda:0')\n",
      "tensor(0.0322, device='cuda:0')\n",
      "tensor(0.0308, device='cuda:0')\n",
      "tensor(0.0295, device='cuda:0')\n",
      "tensor(0.0666, device='cuda:0')\n",
      "tensor(0.0354, device='cuda:0')\n",
      "tensor(0.2484, device='cuda:0')\n",
      "tensor(0.0778, device='cuda:0')\n",
      "tensor(0.0329, device='cuda:0')\n",
      "tensor(0.0370, device='cuda:0')\n",
      "tensor(0.0305, device='cuda:0')\n",
      "tensor(0.0304, device='cuda:0')\n",
      "tensor(0.0321, device='cuda:0')\n",
      "tensor(0.0658, device='cuda:0')\n",
      "tensor(0.2270, device='cuda:0')\n",
      "tensor(0.0740, device='cuda:0')\n",
      "tensor(0.0317, device='cuda:0')\n",
      "tensor(0.0292, device='cuda:0')\n",
      "tensor(0.0391, device='cuda:0')\n",
      "tensor(0.2325, device='cuda:0')\n",
      "tensor(0.0287, device='cuda:0')\n",
      "tensor(0.0316, device='cuda:0')\n",
      "tensor(0.2434, device='cuda:0')\n",
      "tensor(0.2712, device='cuda:0')\n",
      "tensor(0.5489, device='cuda:0')\n",
      "tensor(0.1231, device='cuda:0')\n",
      "tensor(0.0285, device='cuda:0')\n",
      "tensor(0.3084, device='cuda:0')\n",
      "tensor(0.0292, device='cuda:0')\n",
      "tensor(0.0295, device='cuda:0')\n",
      "tensor(0.0397, device='cuda:0')\n",
      "tensor(0.0837, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b9cdb6086f4bcda4399bcb464a021f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation iteration:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0344, device='cuda:0')\n",
      "tensor(0.3329, device='cuda:0')\n",
      "tensor(0.2334, device='cuda:0')\n",
      "tensor(0.0631, device='cuda:0')\n",
      "tensor(0.5825, device='cuda:0')\n",
      "tensor(0.0303, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa454440ac934264a0777570651aad44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation iteration:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4506, device='cuda:0')\n",
      "tensor(0.4022, device='cuda:0')\n",
      "tensor(0.0678, device='cuda:0')\n",
      "tensor(0.3830, device='cuda:0')\n",
      "tensor(0.0833, device='cuda:0')\n",
      "tensor(0.0380, device='cuda:0')\n",
      "Training performance: (0.9621409921671018, 0.9621409921671018, 0.9621409921671018, None)\n",
      "Development performance: (0.9302325581395349, 0.9302325581395349, 0.9302325581395349, None)\n",
      "Test performance: (0.9157894736842105, 0.9157894736842105, 0.9157894736842105, None)\n",
      "Test loss:  0.23747440924247107\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         yes       0.84      0.94      0.89        33\n",
      "          no       0.97      0.90      0.93        62\n",
      "\n",
      "    accuracy                           0.92        95\n",
      "   macro avg       0.90      0.92      0.91        95\n",
      "weighted avg       0.92      0.92      0.92        95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_state_dict = torch.load(os.path.join(OUTPUT_DIR, MODEL_FILE_NAME), map_location=lambda storage, loc: storage)\n",
    "model = BertForSequenceClassification.from_pretrained(BERT_MODEL, state_dict=model_state_dict, num_labels = len(target_names))\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "train_loss, train_correct, train_predicted = evaluate(model, train_dataloader)\n",
    "dev_loss, dev_correct, dev_predicted = evaluate(model, dev_dataloader)\n",
    "test_loss, test_correct, test_predicted = evaluate(model, test_dataloader)\n",
    "\n",
    "print(\"Training performance:\", precision_recall_fscore_support(train_correct, train_predicted, average=\"micro\"))\n",
    "print(\"Development performance:\", precision_recall_fscore_support(dev_correct, dev_predicted, average=\"micro\"))\n",
    "print(\"Test performance:\", precision_recall_fscore_support(test_correct, test_predicted, average=\"micro\"))\n",
    "print(\"Test loss: \", test_loss)\n",
    "\n",
    "bert_accuracy = np.mean(test_predicted == test_correct)\n",
    "\n",
    "print(classification_report(test_correct, test_predicted, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab944b1-939c-494a-9782-c92f51a0cc9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
