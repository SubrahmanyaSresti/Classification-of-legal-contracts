{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                tag  \\\n",
       "0                   ['obligation']   \n",
       "1                   ['obligation']   \n",
       "2                   ['obligation']   \n",
       "3                   ['obligation']   \n",
       "4                   ['obligation']   \n",
       "..                             ...   \n",
       "942  ['prohibition', 'obligation']   \n",
       "943                 ['permission']   \n",
       "944                 ['permission']   \n",
       "945                ['prohibition']   \n",
       "946                ['prohibition']   \n",
       "\n",
       "                                              sentence  \n",
       "0    we will issue a certificate of completion for ...  \n",
       "1    elephant talk bear the risk of and shall indem...  \n",
       "2    subject to the term and condition of this agre...  \n",
       "3    ediets shall ensure that the ediets content co...  \n",
       "4    auriemma will participate in one recording ses...  \n",
       "..                                                 ...  \n",
       "942  a party shall not unreasonably withhold or del...  \n",
       "943  except a otherwise provided if performance her...  \n",
       "944  however if a force majeure event interferes wi...  \n",
       "945  neither party shall make any warranty or repre...  \n",
       "946  no failure or forbearance by either party to e...  \n",
       "\n",
       "[947 rows x 2 columns]>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load your dataset\n",
    "file_path = \"/Users/lalitaneeharikavajjhala/Desktop/Research credits /Data/preprocessed_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize the sentences in your dataset\n",
    "sentences = []\n",
    "for sentence in df['sentence']:\n",
    "    tokens = word_tokenize(sentence)\n",
    "    sentences.append(tokens)\n",
    "\n",
    "# Train the Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences, min_count=1, vector_size=100, window=5)\n",
    "\n",
    "# Save the trained model to a file\n",
    "word2vec_model.save(\"/Users/lalitaneeharikavajjhala/Desktop/Research credits /Models/word2vec_model.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_31 (Embedding)    (None, 200, 100)          292700    \n",
      "                                                                 \n",
      " conv1d_40 (Conv1D)          (None, 200, 256)          77056     \n",
      "                                                                 \n",
      " conv1d_41 (Conv1D)          (None, 200, 256)          196864    \n",
      "                                                                 \n",
      " global_max_pooling1d_23 (G  (None, 256)               0         \n",
      " lobalMaxPooling1D)                                              \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 512)               131584    \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 699743 (2.67 MB)\n",
      "Trainable params: 407043 (1.55 MB)\n",
      "Non-trainable params: 292700 (1.12 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.6840 - accuracy: 0.3719 - val_loss: 0.6724 - val_accuracy: 0.3421 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "5/5 [==============================] - 1s 132ms/step - loss: 0.6745 - accuracy: 0.3603 - val_loss: 0.6698 - val_accuracy: 0.3487 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "5/5 [==============================] - 1s 125ms/step - loss: 0.6698 - accuracy: 0.3736 - val_loss: 0.6644 - val_accuracy: 0.3289 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "5/5 [==============================] - 1s 119ms/step - loss: 0.6662 - accuracy: 0.3554 - val_loss: 0.6582 - val_accuracy: 0.3487 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "5/5 [==============================] - 1s 120ms/step - loss: 0.6605 - accuracy: 0.3653 - val_loss: 0.6533 - val_accuracy: 0.3355 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "5/5 [==============================] - 1s 120ms/step - loss: 0.6570 - accuracy: 0.3669 - val_loss: 0.6494 - val_accuracy: 0.3684 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "5/5 [==============================] - 1s 121ms/step - loss: 0.6546 - accuracy: 0.3636 - val_loss: 0.6490 - val_accuracy: 0.3355 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "5/5 [==============================] - 1s 122ms/step - loss: 0.6539 - accuracy: 0.3917 - val_loss: 0.6458 - val_accuracy: 0.3750 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "5/5 [==============================] - 1s 127ms/step - loss: 0.6511 - accuracy: 0.3752 - val_loss: 0.6488 - val_accuracy: 0.3092 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "5/5 [==============================] - 1s 126ms/step - loss: 0.6551 - accuracy: 0.3554 - val_loss: 0.6465 - val_accuracy: 0.3947 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "5/5 [==============================] - 1s 126ms/step - loss: 0.6472 - accuracy: 0.3934 - val_loss: 0.6460 - val_accuracy: 0.4539 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "5/5 [==============================] - 1s 127ms/step - loss: 0.6475 - accuracy: 0.3554 - val_loss: 0.6474 - val_accuracy: 0.3882 - lr: 0.0010\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.6505 - accuracy: 0.4105\n",
      "Test Loss: 0.6505, Test Accuracy: 0.4105\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted Tags: [('prohibition',)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Loading preprocessed dataset\n",
    "file_path = \"/Users/lalitaneeharikavajjhala/Desktop/Research credits /Data/preprocessed_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Converting tags from strings to lists\n",
    "df['tag'] = df['tag'].apply(lambda x: literal_eval(x))\n",
    "\n",
    "# Encode tags 'y'\n",
    "y = df['tag']\n",
    "multilabel = MultiLabelBinarizer()\n",
    "y = multilabel.fit_transform(y)\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['sentence'])\n",
    "\n",
    "# Convert text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(df['sentence'])\n",
    "\n",
    "# Padding sequences\n",
    "maxlen = 200\n",
    "X = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=df['sentence'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Prepare embedding matrix\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=True, stratify=y)\n",
    "\n",
    "# Define CNN model with additional layers and hyperparameters\n",
    "num_filters = 256\n",
    "kernel_sizes = [3, 4, 5]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=100, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
    "model.add(Conv1D(num_filters, 3, activation='relu', padding='same'))\n",
    "model.add(Conv1D(num_filters, 3, activation='relu', padding='same'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(y.shape[1], activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(),\n",
    "    EarlyStopping(patience=4)\n",
    "]\n",
    "\n",
    "# Train the model with more epochs and batch size\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=128, callbacks=callbacks, validation_split=0.2)\n",
    "\n",
    "# Evaluate model\n",
    "metrics = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss: {:.4f}, Test Accuracy: {:.4f}\".format(metrics[0], metrics[1]))\n",
    "\n",
    "# Predicting\n",
    "x = [\"The confidentiality obligations contained in this section XI shall not apply to the extent that the receiving Party (the 'Recipient') is required (a) to disclose information by law, order or regulation of a governmental agency or a court of competent jurisdiction , or (b) to disclose information to any governmental agency for purposes of obtaining approval to test or market a Product , provided in either case that the Recipient shall provide written notice thereof to the other Party and sufficient opportunity to object to any such disclosure or to request confidential treatment thereof.\"]\n",
    "xt = tokenizer.texts_to_sequences(x)\n",
    "xt = pad_sequences(xt, maxlen=maxlen)\n",
    "prediction = model.predict(xt)\n",
    "\n",
    "probas = (prediction > 0.5).astype(int)\n",
    "tags = multilabel.inverse_transform(probas)\n",
    "\n",
    "print(\"Predicted Tags:\", tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
