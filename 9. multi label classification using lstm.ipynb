{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTING LIBRARIES "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. DATA HANDLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. DATA PRE PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten, GlobalMaxPool1D, Conv1D, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import average_precision_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. SAVING THE MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading preprocessed dataset\n",
    "file_path = \"/Users/lalitaneeharikavajjhala/Desktop/Research credits /Data/preprocessed_data.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['obligation']</td>\n",
       "      <td>we will issue a certificate of completion for each manager trainee who completes the initial training program we require to our satisfaction each such person will be referred to a a certified manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['obligation']</td>\n",
       "      <td>elephant talk bear the risk of and shall indemnify against high usage fraud and bed of it elephant talk customer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['obligation']</td>\n",
       "      <td>subject to the term and condition of this agreement aimmune shall be responsible for the development of the product a set forth herein aimmune itself or with or through it affiliate and sublicensees shall use commercially reasonable effort to perform the development activity for the product to i achieve the development milestone set forth in section and ii obtain regulatory approval for the product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['obligation']</td>\n",
       "      <td>ediets shall ensure that the ediets content complies with editorial guideline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['obligation']</td>\n",
       "      <td>auriemma will participate in one recording session annually during the service period of not more than two hour not including travel time to record a radio advertising spot at a date and location to be mutually agreed upon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tag  \\\n",
       "0  ['obligation']   \n",
       "1  ['obligation']   \n",
       "2  ['obligation']   \n",
       "3  ['obligation']   \n",
       "4  ['obligation']   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                            sentence  \n",
       "0                                                                                                                                                                                                            we will issue a certificate of completion for each manager trainee who completes the initial training program we require to our satisfaction each such person will be referred to a a certified manager  \n",
       "1                                                                                                                                                                                                                                                                                                   elephant talk bear the risk of and shall indemnify against high usage fraud and bed of it elephant talk customer  \n",
       "2  subject to the term and condition of this agreement aimmune shall be responsible for the development of the product a set forth herein aimmune itself or with or through it affiliate and sublicensees shall use commercially reasonable effort to perform the development activity for the product to i achieve the development milestone set forth in section and ii obtain regulatory approval for the product  \n",
       "3                                                                                                                                                                                                                                                                                                                                      ediets shall ensure that the ediets content complies with editorial guideline  \n",
       "4                                                                                                                                                                                     auriemma will participate in one recording session annually during the service period of not more than two hour not including travel time to record a radio advertising spot at a date and location to be mutually agreed upon  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview data head and extend the max column width\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting tags from strings to lists\n",
    "df['tag'] = df['tag'].apply(lambda x: literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding tags 'y'\n",
    "y = df['tag']\n",
    "multilabel = MultiLabelBinarizer()\n",
    "y = multilabel.fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X and y\n",
    "X = list(df.sentence)\n",
    "y = multilabel.transform(df.tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL DEVELOPMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard keras pre-processing\n",
    "maxlen = 200 # Highest word count is 691 and mean is 52; however, 691 is an outlier\n",
    "max_words = 5000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding - sequences with word count less than 200 are added\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((757, 200), (757, 3))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating word embeddings using law2vec\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "law2vec_file = open(\"/Users/lalitaneeharikavajjhala/Desktop/Research credits /Data/Law2Vec.100d.txt\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing each line and store word-vector pairs in a dictionary\n",
    "for line in law2vec_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "law2vec_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each row corresponds to a word with its 100-d word vector\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "# tokenizer.word_index is a list of (word, id) tuples\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 200)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 200, 100)          271900    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               117248    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 389535 (1.49 MB)\n",
      "Trainable params: 117635 (459.51 KB)\n",
      "Non-trainable params: 271900 (1.04 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building RNN model with 128 LSTM units\n",
    "deep_inputs = Input(shape=(maxlen,))\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False, mask_zero=True)(deep_inputs)\n",
    "LSTM_Layer_1 = LSTM(128)(embedding_layer)\n",
    "dense_layer_1 = Dense(3, activation='sigmoid')(LSTM_Layer_1)\n",
    "model = Model(inputs=deep_inputs, outputs=dense_layer_1)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['binary_accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "19/19 [==============================] - 4s 132ms/step - loss: 0.6653 - binary_accuracy: 0.6176 - val_loss: 0.6612 - val_binary_accuracy: 0.6316 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 2s 101ms/step - loss: 0.6428 - binary_accuracy: 0.6281 - val_loss: 0.6317 - val_binary_accuracy: 0.6404 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 2s 101ms/step - loss: 0.6343 - binary_accuracy: 0.6353 - val_loss: 0.6509 - val_binary_accuracy: 0.6294 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 2s 101ms/step - loss: 0.6157 - binary_accuracy: 0.6612 - val_loss: 0.6104 - val_binary_accuracy: 0.6820 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 2s 101ms/step - loss: 0.5636 - binary_accuracy: 0.7152 - val_loss: 0.6061 - val_binary_accuracy: 0.6469 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 2s 103ms/step - loss: 0.5535 - binary_accuracy: 0.7201 - val_loss: 0.6102 - val_binary_accuracy: 0.6557 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 2s 101ms/step - loss: 0.5463 - binary_accuracy: 0.7410 - val_loss: 0.5925 - val_binary_accuracy: 0.6886 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 2s 102ms/step - loss: 0.5109 - binary_accuracy: 0.7587 - val_loss: 0.5355 - val_binary_accuracy: 0.7544 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 2s 101ms/step - loss: 0.4697 - binary_accuracy: 0.7906 - val_loss: 0.5756 - val_binary_accuracy: 0.7237 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 2s 101ms/step - loss: 0.5362 - binary_accuracy: 0.7366 - val_loss: 0.5828 - val_binary_accuracy: 0.7039 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 2s 101ms/step - loss: 0.4607 - binary_accuracy: 0.7983 - val_loss: 0.5547 - val_binary_accuracy: 0.7632 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 2s 101ms/step - loss: 0.4589 - binary_accuracy: 0.8039 - val_loss: 0.5463 - val_binary_accuracy: 0.7697 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(), \n",
    "    EarlyStopping(patience=4)\n",
    "]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=20,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 32ms/step - loss: 0.5227 - binary_accuracy: 0.7614\n",
      "loss: 0.5227085947990417\n",
      "binary_accuracy: 0.7614035606384277\n"
     ]
    }
   ],
   "source": [
    "metrics = model.evaluate(X_test, y_test)\n",
    "print(\"{}: {}\".format(model.metrics_names[0], metrics[0]))\n",
    "print(\"{}: {}\".format(model.metrics_names[1], metrics[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 32ms/step\n",
      "Precision: 0.7386970187820587\n",
      "Recall: 0.6046511627906976\n",
      "F1 Score: 0.6567675191376329\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "thresholded_preds = (y_pred > 0.5).astype(int)  # Applying threshold for binary classification\n",
    "precision = precision_score(y_test, thresholded_preds, average = 'weighted')\n",
    "recall = recall_score(y_test, thresholded_preds, average = 'weighted')\n",
    "f1 = f1_score(y_test, thresholded_preds, average= 'weighted')\n",
    "# print(\"Precision Score: {:.2}\".format(average_precision_score(y_test,y_pred)))\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'x = [\"Each Party shall return to the other all of the other’s Confidential Information and any other material, information or samples relating to the Product which have been provided or made available to the other and shall not retain any copies and the Parties further agree not to make any further use of each other’s Confidential Information or any other information, data or samples relating to the Product provided or made available by the other Party, except as necessary to comply with its statutory, regulatory or licensing obligations; provided, however, that Kitov may retain such material, information and/or samples relating to the Product as may be necessary for Kitov to continue to sell the Product as permitted by Section \\u200b5.4.4 below, following which, Kitov shall refrain from making any further use of Dexcel’s Confidential Information or any other information, data or samples and shall return any remaining Confidential Information and material, information or samples relating to the Product.\"]\\nprediction = model.predict(x)\\n\\nprobas = np.array(prediction)\\nlabels = (probas > 0.5).astype(np.int)\\ntags = multilabel.inverse_transform(labels)\\n\\nprint(prediction)\\nprint(labels)\\nprint(tags)\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''x = [\"Each Party shall return to the other all of the other’s Confidential Information and any other material, information or samples relating to the Product which have been provided or made available to the other and shall not retain any copies and the Parties further agree not to make any further use of each other’s Confidential Information or any other information, data or samples relating to the Product provided or made available by the other Party, except as necessary to comply with its statutory, regulatory or licensing obligations; provided, however, that Kitov may retain such material, information and/or samples relating to the Product as may be necessary for Kitov to continue to sell the Product as permitted by Section ​5.4.4 below, following which, Kitov shall refrain from making any further use of Dexcel’s Confidential Information or any other information, data or samples and shall return any remaining Confidential Information and material, information or samples relating to the Product.\"]\n",
    "prediction = model.predict(x)\n",
    "\n",
    "probas = np.array(prediction)\n",
    "labels = (probas > 0.5).astype(np.int)\n",
    "tags = multilabel.inverse_transform(labels)\n",
    "\n",
    "print(prediction)\n",
    "print(labels)\n",
    "print(tags)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVING THE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/lalitaneeharikavajjhala/Desktop/Research credits /Models/MultiLabelModel_LSTM.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "joblib.dump(model, \"/Users/lalitaneeharikavajjhala/Desktop/Research credits /Models/MultiLabelModel_LSTM.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
